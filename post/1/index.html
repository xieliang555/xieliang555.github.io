<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.55.5" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="" />
  <meta property="og:url" content="https://gohugo.io/post/1/" />
  <link rel="canonical" href="https://gohugo.io/post/1/" /><script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gohugo.io\/"
      },
      "articleSection" : "post",
      "name" : "REINFORCE: Monte Carlo Policy Gradient",
      "headline" : "REINFORCE: Monte Carlo Policy Gradient",
      "description" : "\x3col\x3e\n\x3cli\x3e最简单的 \x3cem\x3ePolicy Gradient\x3c\/em\x3e 的公式推导\x3cbr \/\x3e\x3c\/li\x3e\n\x3cli\x3e\x3cem\x3ebasic REINFORCE\x3c\/em\x3e 的方差\x3c\/li\x3e\n\x3cli\x3e引入“reward-to-go”和“advantage centering”减小样本方差。\x3c\/li\x3e\n\x3c\/ol\x3e",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2019",
      "datePublished": "2019-06-16 20:47:39 \x2b0800 CST",
      "dateModified" : "2019-06-16 20:47:39 \x2b0800 CST",
      "url" : "https:\/\/gohugo.io\/post\/1\/",
      "keywords" : [  ]
  }
</script>
<title>REINFORCE: Monte Carlo Policy Gradient - XieLiang</title>
  <meta property="og:title" content="REINFORCE: Monte Carlo Policy Gradient - XieLiang" />
  <meta property="og:type" content="article" />
  <meta name="description" content="
最简单的 Policy Gradient 的公式推导
basic REINFORCE 的方差
引入“reward-to-go”和“advantage centering”减小样本方差。
" />

  <link rel="stylesheet" href="/css/flexboxgrid-6.3.1.min.css" />
  <link rel="stylesheet"
    href="/css/github-markdown.min.css" />
  <link rel="stylesheet" href="/css/highlight/tomorrow.min.css" />
  <link rel="stylesheet" href="/css/index.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="XieLiang">
  
  <link href="https://fonts.googleapis.com/css?family=Arvo|Permanent+Marker" rel="stylesheet">
  
  

  
</head>


<body>
  <article class="post " id="article">
    <div class="row">
      <div class="col-xs-12 col-sm-10 col-md-8 col-sm-offset-1 col-md-offset-2 col-lg-6 col-lg-offset-3">
        <div class="site-header">
          <header id="header" data-behavior="">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="/">XieLiang</a>
  </div>
  

  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

  
</header>


        </div>
        <header class="post-header">
          <h1 class="post-title">REINFORCE: Monte Carlo Policy Gradient</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2019-06-16 20:47:39 CST">
                16 Jun 2019
              </time>
              
            </div>
            <div class="col-xs-6">
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          <ol>
<li>最简单的 <em>Policy Gradient</em> 的公式推导<br /></li>
<li><em>basic REINFORCE</em> 的方差</li>
<li>引入“reward-to-go”和“advantage centering”减小样本方差。</li>
</ol>

<h4 id="pg公式推导">PG公式推导：</h4>

<p>假设一段轨迹 <em>trajectory</em> $\tau =\{s_0, a_0, s_1, a_1&hellip;s_{T+1}\}$ 在参数化的策略 <em>policy</em> $\pi_{\theta}$下的总奖励表示为: $R(\tau)=\sum_{t=0}^Tr(t)$。目标是最大化 $R(\tau)$的期望：
$J(\theta)=\underset{\tau \sim \pi_{\theta} }E[R(\tau)]$。</p>

<p>由于通过$\pi_{\theta}$ ($\theta$表示神经网络的参数) 将 <em>policy</em> 参数化，故可以利用梯度上升更新参数，优化 <em>policy</em> ：
$$\theta_{k+1}=\theta_{k}+\alpha\nabla_{\theta}J(\theta)|_{\theta_k}$$</p>

<p>随机选择初始状态 $s_0$，在 <em>policy</em> $\pi_{\theta}$下，$\tau$的概率表示为：<br />
$$P(\tau|\theta)=\rho(s_0)\underset{t=0}{\overset{T}{\Pi}}P(s_{t+1}|s_t,a_t)\pi_{\theta}(a_t|s_t)$$<br />
其中$\pi_{\theta}(a_t|s_t)$表示在状态 $s_t$下，根据 <em>policy</em> $\pi_{\theta}$ 选择动作 $a_t$的概率。</p>

<p>梯度 $\nabla_{\theta}J(\theta)$:
$$
\begin{split}
\nabla_{\theta}J(\theta) &amp;=\nabla_{\theta}\underset{\tau\sim\pi_{\theta}}{E}[R(\tau)]\\<br />
                          &amp;=\nabla_{\theta}\int_{\tau}P(\tau|\theta)R(\tau)\\
                          &amp;=\int_{\tau}P(\tau|\theta)\nabla_{\theta}logP(\tau|\theta)R(\tau)\\
                          &amp;=\int_{\tau}P(\tau|\theta)\sum_{t=0}^T\nabla_{\theta}log\pi_{\theta}(a_t|s_t)R(\tau)\\
                          &amp;=\underset{\tau\sim\pi_{\theta}}{E}\left[\sum_{t=0}^T\nabla_{\theta}log\pi_{\theta}(a_t|s_t)R(\tau)\right]
\end{split}
$$</p>

<p><strong>Note：</strong>   <em>REINFORCE</em> 使用整段 <em>trajectory</em> 的$R(\tau)$来更新参数，参数的每一步更新在 <em>trajectory</em> 结束后开始，故将 <em>REINFORCE</em> 归为 <em>Monte Carlo</em> 算法(区别于 <em>Temporal-Difference</em> 算法)。</p>

<h4 id="方差">方差：</h4>

<p>首先回顾 <em>baisc REINFORCE</em> 中的梯度表达式：
$$
\begin{split}
\nabla_{\theta}J(\theta) =\underset{\tau\sim\pi_{\theta}}{E}\left[\sum_{t=0}^T\nabla_{\theta}log\pi_{\theta}(a_t|s_t)R(\tau)\right]=\underset{\tau\sim\pi_{\theta}}{E}\left[\nabla_{\theta}log\pi_{\theta}(\tau)R(\tau)\right]
\end{split}
$$</p>

<p><em>basic REINFORCE</em>  属于 <em>stochastic policy gradient</em> 方法，因此 $\nabla_{\theta}log\pi_{\theta}(\tau)R(\tau)$是一个随机变量($\tau$是随机变量，见下图)，<em>policy gradient</em> 中所说的方差就是指 $Var[\nabla_{\theta}log\pi_{\theta}(\tau)R(\tau)]$。
<div align=center><img src="/1/1.png" alt="optimal" /></div>
<font size=2 face="黑体">(图片来自CS294)</font></p>

<p>通常为了计算梯度$\nabla_{\theta}J(\theta)$，我们会在当前 <em>policy</em> ($\tau\sim\pi_{\theta}$)下，采样 $N$段 <em>trajectory</em> 并求平均：
$$\nabla_{\theta}J(\theta)\approx\frac{1}{N}\sum_{i=1}^N\nabla_{\theta}log\pi_{\theta}(\tau_i)R(\tau_i)$$
当然，当 $N\to\infty$，我们估计的梯度值逐渐趋近于期望值，但限于计算资源 $N$ 不能取太大。如果一个随机变量具有较小的方差，那么只需要取较小的 $N$，其样本估计值就会比较接近期望值。</p>

<h4 id="reward-to-go">Reward-to-go :</h4>

<p>作为 <em>Monte Carlo</em> 方法，<em>basic REINFORCE</em> 具有较高的方差，可以通过增大 <em>batch size</em> 来减小方差，也可以通过减小 <em>learning rate</em> 来保证收敛，但是这都会降低学习效率。</p>

<p><em>basic REINFORCE</em> 中每一个$\pi_{\theta}(a_t|s_t)$ 和$r_t$都是取决于 $\tau$ 的随机变量，导致方差很大。可以通过减小一些随机变量来减小方差。于是之前的梯度表达式：
$$\nabla_{\theta}J(\theta)=\underset{\tau\sim\pi_{\theta}}{E}\left[\sum_{t=0}^T\nabla_{\theta}log\pi_{\theta}(a_t|s_t)\sum_{t=0}^Tr_t\right]$$</p>

<p>可以改写成：
$$\nabla_{\theta}J(\theta)=\underset{\tau\sim\pi_{\theta}}{E}\left[\sum_{t=0}^T\nabla_{\theta}log\pi_{\theta}(a_t|s_t)\sum_{t&rsquo;=t}^Tr_{t&rsquo;}\right]$$</p>

<p><font size=2 face="黑体"><strong>注意:</strong> $t=0\rightarrow t&rsquo;=t$</font></p>

<p>直观上看（参考 <a href="/post/2/">Policy Gradient公式背后的直观解释</a>），之前积累的奖励不会影响当前的梯度，因此可以直接去掉，这样就减少了很多随机变量。直接去掉这些随机变量不会引入偏差，但是可以减小方差。(<a href="https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html">proof</a>)</p>

<h4 id="advantage-centering">Advantage centering:</h4>

<p>对 <em>policy gradient</em> 中的 <em>Advantage</em> 项标准化叫做 <em>Advantage centering</em> 。如:
$$R(\tau_i)\leftarrow\frac{R(\tau_i)-mean(R(\tau))}{std(R(\tau))}$$</p>

<p>举一个简单的例子，现在有3段 $\tau$，其 $\nabla_{\theta}log\pi_{\theta}(\tau)$ 分别为 (0.5,0.2,0.3) (为简化考虑$\theta$是标量)，$R(\tau)$分别为(1000,1001,1002)，则方差： $Var[\nabla_{\theta}log\pi_{\theta}(\tau)R(\tau)]=$23286.76，标准化后 $R(\tau)=$(1.3 -1.1 -0.3)，方差：
$Var[\nabla_{\theta}log\pi_{\theta}(\tau)R(\tau)]=$0.22</p>

<p><em>Advantage centering</em> 不会引入偏差，但是可以减小方差。</p>

<h4 id="demo">Demo:</h4>

<p><a href="https://github.com/xieliang555/CS294.git">git repo</a></p>

<p>下图中，将三种方法 <em>basic REINFORCE</em>, <em>reward to go PG</em> 和 <em>advanatge centering</em> 进行对比。其中图一和图二分别是 <em>small batch</em> 和 <em>large batch</em> 。对于不同 <em>batch size</em> ，当 <em>learning rate</em> 相同时， <em>reward to go PG</em> 和 <em>advanatge centering</em> 收敛的更快。图一和图二对比，<em>batch size</em> 越大，收敛越快。
<div align=center><img src="/1/3.png" alt="optimal" /></div>
<div align=center><img src="/1/2.png" alt="optimal" /></div>
<font size=2 face="黑体"> <strong>Note:</strong> sb: small batch, no_rtg: no reward to go, dna: don&rsquo;t normalize advantage, lb: large batch</font></p>
        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
          </div>
        </div>

        
        

        
        
        <div style="height: 50px;"></div>
        
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  <script src="/js/highlight.pack.js"></script>


<script>
  hljs.initHighlightingOnLoad();
  
  
  
    
    
  
</script>

  

</body>

</html>