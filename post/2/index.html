<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Hugo 0.55.5" />

  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="author" content="" />
  <meta property="og:url" content="https://gohugo.io/post/2/" />
  <link rel="canonical" href="https://gohugo.io/post/2/" /><script type="application/ld+json">
  {
      "@context" : "http://schema.org",
      "@type" : "BlogPosting",
      "mainEntityOfPage": {
           "@type": "WebPage",
           "@id": "https:\/\/gohugo.io\/"
      },
      "articleSection" : "post",
      "name" : "Policy Gradient公式背后的直观解释",
      "headline" : "Policy Gradient公式背后的直观解释",
      "description" : "",
      "inLanguage" : "en-US",
      "author" : "",
      "creator" : "",
      "publisher": "",
      "accountablePerson" : "",
      "copyrightHolder" : "",
      "copyrightYear" : "2019",
      "datePublished": "2019-06-29 14:17:10 \x2b0800 CST",
      "dateModified" : "2019-06-29 14:17:10 \x2b0800 CST",
      "url" : "https:\/\/gohugo.io\/post\/2\/",
      "keywords" : [ "tag1","tag2", ]
  }
</script>
<title>Policy Gradient公式背后的直观解释 - Hugo</title>
  <meta property="og:title" content="Policy Gradient公式背后的直观解释 - Hugo" />
  <meta property="og:type" content="article" />
  <meta name="description" content="" />

  <link rel="stylesheet" href="/css/flexboxgrid-6.3.1.min.css" />
  <link rel="stylesheet"
    href="/css/github-markdown.min.css" />
  <link rel="stylesheet" href="/css/highlight/tomorrow.min.css" />
  <link rel="stylesheet" href="/css/index.css">
  <link href="/index.xml" rel="alternate" type="application/rss+xml" title="Hugo">
  
  <link href="https://fonts.googleapis.com/css?family=Arvo|Permanent+Marker" rel="stylesheet">
  
  

  
</head>


<body>
  <article class="post " id="article">
    <div class="row">
      <div class="col-xs-12 col-sm-10 col-md-8 col-sm-offset-1 col-md-offset-2 col-lg-6 col-lg-offset-3">
        <div class="site-header">
          <header id="header" data-behavior="">
  <i id="btn-open-sidebar" class="fa fa-lg fa-bars"></i>
  <div class="header-title">
    <a class="header-title-link" href="/">Hugo</a>
  </div>
  

  <script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>

  
</header>


        </div>
        <header class="post-header">
          <h1 class="post-title">Policy Gradient公式背后的直观解释</h1>
          
          <div class="row post-desc">
            <div class="col-xs-6">
              
              <time class="post-date" datetime="2019-06-29 14:17:10 CST">
                29 Jun 2019
              </time>
              
            </div>
            <div class="col-xs-6">
              
            </div>
          </div>
          
        </header>

        <div class="post-content markdown-body">
          <h4 id="pg公式背后的直观解释">PG公式背后的直观解释：</h4>

<p><a href="https://github.com/xieliang555/intuitive_policy_gradient">git repo</a></p>

<p>首先，我们的目标是不断更新 $\theta$，直到 $\pi_{\theta}$收敛到最优 <em>policy</em> 。对于离散动作空间，$\pi_{\theta}$ 可以是一个 <em>MLP</em> 神经网络（最后一层是 <em>softmax</em> ），那么神经网络的输出可以看作是 每一个动作的概率，即 $\pi_{\theta}(a_t|s_t)$。</p>

<p>很显然，如果动作 $a^*$ 是在状态 $s$下的最优动作，那么我们希望 $\pi_{\theta}(a^*|s)$尽可能的接近1。现在假设离散动作空间的维度是3， 并已知状态 $s$下的最优动作是第一个动作，那么只需要利用梯度上升更新 $\theta$，就可以得到最优 <em>policy</em>:
$$\theta_{t+1}=\theta_{t}+\alpha\nabla_{\theta}\pi_{\theta}(a^*|s)|_{\theta_{t}}$$
<div align=center><img src="/2/1.png" alt="optimal" /><div align=left>
<div align=center><img src="/2/random_action.gif" alt="optimal" /><div align=left>
上面的 <em>gif</em> 图中，每一个 <em>bar</em> 的高度表示在状态 <em>s</em> 下对应动作的概率。由于目前只以增大第一个动作概率为目标更新参数，故只有第一个 <em>bar</em> 的概率增大，而其他概率减小。</p>

<p>但是，我们目前不知道哪个动作是最优的，毕竟这是最终需要求的。因此也不知道要以哪个动作的概率为目标更新参数。一个方法是每次迭代时，随机选择一个动作的概率作为目标，但是对不同动作的更新幅度赋予不同的权重 ，$\hat{Q}(s,a)$表示在状态 $s$ 下动作 $a$ 的 <em>value</em> ，可以通过样本估计获得。更优的动作 <em>value</em> 更高，因此在相同更新次数(随机选择动作)的情况下，相对更优的动作的概率增长幅度更大，那么最终也能收敛到最优 <em>policy</em> 。
$$\theta_{t+1}=\theta_{t}+\alpha \hat{Q}(s,a)\nabla_{\theta}\pi_{\theta}(a|s)|_{\theta_{t}}$$
<div align=center><img src="/2/2.png" alt="optimal" /><div align=left>
<div align=center><img src="/2/random_action.gif" alt="optimal" /><div align=left></p>

<blockquote>
<p>为什么要减小梯度估计的样本方差？</p>
</blockquote>

<p>为了使神经网络的参数更新更稳定，收敛更快。</p>

<blockquote>
<p>如何减小方差及原因（证明）：</p>
</blockquote>

<p>(1) 更大的batch，更小的learning rate</p>

<p>(2) reward-to-go:<br />
But how is this better? A key problem with policy gradients is how many sample trajectories are needed to get a low-variance sample estimate for them. The formula we started with included terms for reinforcing actions proportional to past rewards, all of which had zero mean, but nonzero variance: as a result, they would just add noise to sample estimates of the policy gradient. By removing them, we reduce the number of sample trajectories needed.</p>

<p>(3) advantage centering</p>

<blockquote>
<p>实验证明：</p>
</blockquote>

<p>average return可以衡量policy gradient的样本方差？</p>

<p>图+代码</p>
        </div>

        <div class="row middle-xs">
          <div class="col-xs-12">
            
            <div class="post-category">
              <a href="/categories/category/">
                category
              </a>
            </div>
            
            <div class="post-category">
              <a href="/categories/subcategory/">
                subcategory
              </a>
            </div>
            
          </div>
        </div>

        
        

<div class="releated-content">
  <h3>Related Posts</h3>
  <ul>
    
    <li><a href="/post/9/">RL伪代码</a></li>
    
    <li><a href="/post/8/">Summary of Notation</a></li>
    
    <li><a href="/post/hugo/">Hugo</a></li>
    
  </ul>
</div>

        
        
        <div style="height: 50px;"></div>
        
        

        <div class="site-footer">
  
  
</div>

      </div>
    </div>
  </article>

  <script src="/js/highlight.pack.js"></script>


<script>
  hljs.initHighlightingOnLoad();
  
  
  
    
    
  
</script>

  

</body>

</html>